{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the input text: 5\n",
      "The subject of the first sentence is animals.\n",
      " This topic is covered by the computer science but electrical and electronics engineers can do it too.\n"
     ]
    }
   ],
   "source": [
    "def euclideanDistance(sample1,sample2):\n",
    "    import math\n",
    "    sum = 0\n",
    "    for i in range(sample1.shape[0]):\n",
    "        number = sample1[i]-sample2[i]\n",
    "        sum = sum + math.pow(number,2)\n",
    "            \n",
    "    return math.pow(sum,0.5)\n",
    "\n",
    "\n",
    "\n",
    "def kMeans(features, input,k):\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    features = features.reshape((features.shape[0],features.shape[1]*features.shape[2]))\n",
    "\n",
    "    model = KMeans(n_clusters=k)\n",
    "    labels = model.fit_predict(features)\n",
    "    cluster_centers = model.cluster_centers_\n",
    "\n",
    "    distance_list = []\n",
    "\n",
    "    for center in list(cluster_centers):\n",
    "        distances = {}\n",
    "        for i, sentence in enumerate(features):\n",
    "            distances[str(i)] = euclideanDistance(center,sentence)\n",
    "\n",
    "        import operator \n",
    "        sorted_distances = sorted(distances.items(), key=operator.itemgetter(1))\n",
    "        distance_list.append(sorted_distances)\n",
    "\n",
    "    idx = [int(value[0][0]) for value in distance_list]\n",
    "    idx = sorted(idx)\n",
    "    \n",
    "    return idx\n",
    "\n",
    "\n",
    "\n",
    "def BERTFeatureExtraction(input, max_token_size):\n",
    "    input = input.split(\".\")\n",
    "    input = input[:-1]\n",
    "    print(\"Number of sentences in the input text:\", len(input))\n",
    "    \n",
    "    \n",
    "    from transformers import BertModel, BertTokenizer\n",
    "    import torch\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    features = []\n",
    "    \n",
    "    for sentence in (input):\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        special_tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        \n",
    "        if len(special_tokens)>max_token_size:\n",
    "            print(\"Warning max_token_size is small. Change the parameter! Maybe be there is no dot in the input text.\")\n",
    "            \n",
    "            \n",
    "        padded_tokens = special_tokens + ['[PAD]' for i in range(max_token_size-len(special_tokens))]\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in padded_tokens]\n",
    "        \n",
    "        token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "        attn_mask = torch.tensor(attn_mask).unsqueeze(0)\n",
    "        \n",
    "        hidden_repr, cls_head = model(token_ids, attention_mask = attn_mask)     \n",
    "        features.append(hidden_repr[0]) #batch dimension reduction (1, token_size, wv) => (token_size, wv)\n",
    "    \n",
    "    \n",
    "    import numpy as np\n",
    "    features = [value.detach().numpy() for value in features]\n",
    "    features = [value.reshape((max_token_size,768)) for value in features]\n",
    "    features = np.array(features).reshape((len(features),max_token_size,768))\n",
    "    \n",
    "    \n",
    "    return features, input\n",
    "\n",
    "\n",
    "#MAIN************************************************************************\n",
    "\n",
    "input = \"The subject of the first sentence is animals. Protecting animals is a very important topic. For instance, dog is a type of animal. Also, another topic is natural language processing. This topic is covered by the computer science but electrical and electronics engineers can do it too.\"\n",
    "\n",
    "\n",
    "max_token_size = 50\n",
    "k=2\n",
    "\n",
    "features, input = BERTFeatureExtraction(input, max_token_size) #input is string, output is (batch_size=num_sen, token_size, hidden_dim)\n",
    "idx = kMeans(features, input, k)\n",
    "\n",
    "\n",
    "summary = [input[id]+\".\" for id in(idx)]\n",
    "for sentence in summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
